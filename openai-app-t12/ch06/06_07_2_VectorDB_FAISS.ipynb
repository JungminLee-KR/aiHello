{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-10T07:37:11.276564Z",
     "start_time": "2025-11-10T07:37:11.259428Z"
    }
   },
   "source": [
    "import os, sys, shutil\n",
    "from xml.dom.minidom import Document\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../_apikeys.env\")\n",
    "api_key = os.getenv(\"DoogieOpenaiKey\")\n",
    "os.environ['OPENAI_API_KEY'] = api_key"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#\n",
    "# Step 1-1: get test documents\n",
    "#\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://github.com/chatgpt-kr/openai-api-tutorial/raw/main/ch06/2023_%EB%B6%81%ED%95%9C%EC%9D%B8%EA%B6%8C%EB%B3%B4%EA%B3%A0%EC%84%9C.pdf\", filename=\"06_07_test.pdf\")"
   ],
   "id": "7dbde76dc7a2f13c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T07:37:32.504034Z",
     "start_time": "2025-11-10T07:37:11.538846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Step 1-2: load and split documents\n",
    "# 17~20 sec\n",
    "#\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader( \"06_07_test.pdf\" )\n",
    "pages = loader.load_and_split()  # about 17s\n",
    "#pages = loader.load()  # about 16s, include empty page\n"
   ],
   "id": "97349a1e9c9a70cd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print( 'type loader,pages:', type(loader), type(pages) )\n",
    "print( 'type pages[0]s:', type(pages[0]), type(pages[0].page_content))\n",
    "print( 'size/len loader,pages:', sys.getsizeof(loader), len(pages) )\n",
    "print( 'size/len pages[0]s:', sys.getsizeof(pages[0]), len(pages[0].page_content) )\n",
    "print( 'pages Max:', max(len(apage.page_content) for apage in pages ) )\n",
    "print( 'pages Min:', min(len(apage.page_content) for apage in pages ) )\n",
    "print( 'pages Sum:', sum(len(apage.page_content) for apage in pages ) )\n",
    "print( '#of chunks:', len(pages) )\n",
    "print( 'Chunk Avg:', sum(len(apage.page_content) for apage in pages ) / len(pages) )"
   ],
   "id": "4642e6abb2a5cd14",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#print( pages[0] )\n",
    "#print( pages[0].page_content)\n",
    "#print( \"example:\", pages[6].page_content[:50] )\n",
    "#print( pages[442].page_content )"
   ],
   "id": "95f7bb8ba1bcbccd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T09:00:12.133681Z",
     "start_time": "2025-11-10T09:00:12.102324Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: case1:\n",
    "# RecursiveCharacterTextSpilter를 사용함\n",
    "#\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "textSplitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size= 1000, # 1000\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "splitedDocs = textSplitter.split_documents( pages )"
   ],
   "id": "3d2bc0cdf6ef0133",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print( 'type split,splitDocs:', type(textSplitter), type(splitedDocs) )\n",
    "print( 'type splitedDocs[0]s:', type(splitedDocs[0]), type(splitedDocs[0].page_content) )\n",
    "print( 'size/len split,splitDocs:', sys.getsizeof(textSplitter), len(splitedDocs) )\n",
    "print( 'size/len splitedDocs[0]s:', sys.getsizeof(splitedDocs[0]), len(splitedDocs[0].page_content) )\n",
    "print( 'splitedDocs Max:', max(len(apage.page_content) for apage in splitedDocs ) )\n",
    "print( 'splitedDocs Min:', min(len(apage.page_content) for apage in splitedDocs ) )\n",
    "print( 'splitedDocs Sum:', sum(len(apage.page_content) for apage in splitedDocs ) )\n",
    "print( '#of chunks:', len(splitedDocs) )\n",
    "print( 'Chunk Avg:', sum(len(apage.page_content) for apage in splitedDocs ) / len(splitedDocs) )"
   ],
   "id": "aecff3c27907fced",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#\n",
    "# DONT USE THIS CODE !!! use Step3-2 Code !!!\n",
    "# Step3-1: Loading splitedDocs to chroma\n",
    "# Error case: \"Requested 367501 tokens, max 300000 tokens per request\"\n",
    "#\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "myEmbeddings = OpenAIEmbeddings()\n",
    "print(\"current OpenAIEmbeddings().model=\", myEmbeddings.model)\n",
    "myEmbeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "print(\"current OpenAIEmbeddings().model=\", myEmbeddings.model)\n",
    "\n",
    "fdb = FAISS.from_documents(\n",
    "    splitedDocs,\n",
    "    myEmbeddings\n",
    ")\n",
    "print(\"vdb적재문서수=\", fdb.index.ntotal)\n"
   ],
   "id": "be42d26e0ae31da0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T09:00:22.023208Z",
     "start_time": "2025-11-10T09:00:21.849929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Step3-2: using package 'langchain_openai', not 'OpenAIEmbeddings'\n",
    "#          from langchain.embeddings import OpenAIEmbeddings\n",
    "#          'langchain.embeddings' deprecated, use 'langchain_openai'\n",
    "#\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "#from langchain.embeddings import OpenAIEmbeddings\n",
    "#import faiss\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "myEmbeddings = OpenAIEmbeddings()\n",
    "print(\"current OpenAIEmbeddings().model=\", myEmbeddings.model)\n",
    "myEmbeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "print(\"current OpenAIEmbeddings().model=\", myEmbeddings.model)\n",
    "\n",
    "faiss_index_path = \"../localdb/my_faiss_db_11\"\n",
    "#myCollectionName = \"my_collection\"\n",
    "\n",
    "if os.path.exists(faiss_index_path):\n",
    "    print(f\"기존 FAISS DB 삭제: {faiss_index_path}\")\n",
    "    shutil.rmtree(faiss_index_path)\n",
    "\n",
    "#\n",
    "# if u first create and add with splitedDocs to myEmbeddings, use Step3-2\n",
    "# but if u want to use already Chroma data, use Step3-3\n",
    "#\n"
   ],
   "id": "3aa52a7a37b35b92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current OpenAIEmbeddings().model= text-embedding-ada-002\n",
      "current OpenAIEmbeddings().model= text-embedding-3-large\n",
      "기존 FAISS DB 삭제: ../localdb/my_faiss_db_11\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T09:00:39.159523Z",
     "start_time": "2025-11-10T09:00:26.549689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Step3-3: Open or ClearInit FAISS\n",
    "#\n",
    "from langchain.schema import Document\n",
    "fdb = None\n",
    "\n",
    "def create_open_faiss(persist_dir, embedding_func):\n",
    "    global fdb\n",
    "\n",
    "    print(f\"[create_open_faiss] FAISS DB ({persist_dir})...\")\n",
    "    if os.path.exists(persist_dir):\n",
    "        print(f\"FAISS DB already exists, loading...: {persist_dir}\")\n",
    "        fdb = FAISS.load_local(\n",
    "            folder_path = persist_dir,\n",
    "            embeddings = embedding_func,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "        print(f\"FAISS DB loaded: {fdb.index.ntotal} documents...\")\n",
    "        return_val = fdb.index.ntotal\n",
    "    else:\n",
    "        print(f\"FAISS DB does not exist, creating...: {persist_dir}\")\n",
    "        if len(splitedDocs) == 0:\n",
    "            print(\"추가할 문서가 없습니다.\")\n",
    "            exit()\n",
    "\n",
    "        # 첫 문서 1개로 초기화\n",
    "        #fdb = FAISS.from_documents([splitedDocs[0]], myEmbeddings)\n",
    "        #print(\"FAISS DB 초기화 완료 (with 첫 1개 문서)\")\n",
    "\n",
    "        # Dummy 문서 1개로 초기화\n",
    "        dummy_doc = Document(page_content=\"dummy initialization\",\n",
    "                             metadata={\"source\": \"init\"})\n",
    "        fdb = FAISS.from_documents([dummy_doc], embedding_func)\n",
    "        dummy_id = list(fdb.docstore._dict.keys())[0]\n",
    "        fdb.delete([dummy_id])\n",
    "        print(f\"빈 FAISS DB 생성 완료 {dummy_id}\")\n",
    "        return_val = 0\n",
    "\n",
    "    print(f\"[create_open_faiss] FAISS DB lendth: ({fdb.index.ntotal}) items...\")\n",
    "    fdb.save_local(faiss_index_path)\n",
    "    return return_val\n",
    "\n",
    "def clear_faiss(persist_dir):\n",
    "    global fdb\n",
    "\n",
    "    if fdb is not None and fdb.index.ntotal > 0:\n",
    "        all_ids = list(fdb.docstore._dict.keys())\n",
    "        if all_ids:\n",
    "            fdb.delete(all_ids)\n",
    "            print(f\"[clear FAISS] 모든 문서 삭제 완료: {len(all_ids)}개\")\n",
    "            print(f\"[clear FAISS] 삭제 후 크기: {fdb.index.ntotal}\")\n",
    "    else:\n",
    "        print(\"[clear FAISS] 삭제할 문서가 없습니다.\")\n",
    "\n",
    "\n",
    "def batch_add_docs(persist_dir, documents, batch_size=100):\n",
    "    global fdb\n",
    "\n",
    "    splitedDocsLen = len(documents)\n",
    "    print(\">> ready to insert splitedDocs, len = \", splitedDocsLen)\n",
    "\n",
    "    if splitedDocsLen == 0:\n",
    "        print(\"splitedDocs(documents)가 비어있습니다.\")\n",
    "        return 0\n",
    "\n",
    "    if fdb is None:\n",
    "        print(\"FAISS DB가 NONE입니다...\")\n",
    "        return 0\n",
    "\n",
    "    batchCount = splitedDocsLen // batch_size\n",
    "    if splitedDocsLen % batch_size > 0:\n",
    "        batchCount += 1\n",
    "    print(\">> batchsize is \", batch_size, \"and batchCount:\", batchCount )\n",
    "\n",
    "    for i in range(0, splitedDocsLen, batch_size):\n",
    "        #print(\">>\", i, \"th batch, \", batch, \"..\", batch + batch_size - 1, end=\"\")\n",
    "        print(f\">>add: { min(i + batch_size, splitedDocsLen)}/{splitedDocsLen}\", end=\"\")\n",
    "        batchDocs = documents[i:i + batch_size]\n",
    "        fdb.add_documents(batchDocs)\n",
    "        print(\" ++ added batchDocs len:\", len(batchDocs))\n",
    "\n",
    "    fdb.save_local(persist_dir)\n",
    "    print(\">> FAISS 인덱스 저장 완료, 위치: \", persist_dir)\n",
    "    print(\">> fdb size:\", len(fdb.index_to_docstore_id) )\n",
    "    print(\">> fdb size:\", fdb.index.ntotal )\n",
    "    return\n",
    "\n",
    "\n",
    "print(f\"[START] FAISS DB ({faiss_index_path}) add documents...:\")\n",
    "count = create_open_faiss(faiss_index_path, myEmbeddings)\n",
    "print(f\"create_open_faiss() return_val={count}\")\n",
    "print(f\"docstore_id={len(fdb.index_to_docstore_id)}, ntotal={fdb.index.ntotal}\" )\n",
    "\n",
    "if count == 0:\n",
    "    batch_add_docs(faiss_index_path, splitedDocs, 100)\n",
    "\n",
    "#\n",
    "# optional: if u want to only clear exist FAISS\n",
    "#\n",
    "#clear_faiss(faiss_index_path)\n"
   ],
   "id": "bdad2c0e20074f8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] FAISS DB (../localdb/my_faiss_db_11) add documents...:\n",
      "[create_open_faiss] FAISS DB (../localdb/my_faiss_db_11)...\n",
      "FAISS DB does not exist, creating...: ../localdb/my_faiss_db_11\n",
      "빈 FAISS DB 생성 완료 772d0e9d-4ce1-4aa0-9eda-6021189dc250\n",
      "[create_open_faiss] FAISS DB lendth: (0) items...\n",
      "create_open_faiss() return_val=0\n",
      "docstore_id=0, ntotal=0\n",
      ">> ready to insert splitedDocs, len =  496\n",
      ">> batchsize is  100 and batchCount: 5\n",
      ">>add: 100/496 ++ added batchDocs len: 100\n",
      ">>add: 200/496 ++ added batchDocs len: 100\n",
      ">>add: 300/496 ++ added batchDocs len: 100\n",
      ">>add: 400/496 ++ added batchDocs len: 100\n",
      ">>add: 496/496 ++ added batchDocs len: 96\n",
      ">> FAISS 인덱스 저장 완료, 위치:  ../localdb/my_faiss_db_11\n",
      ">> fdb size: 496\n",
      ">> fdb size: 496\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T09:00:47.766832Z",
     "start_time": "2025-11-10T09:00:47.746936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Step3-4: open FAISS and use\n",
    "#\n",
    "faissdb = FAISS.load_local(\n",
    "    folder_path = faiss_index_path,\n",
    "    embeddings = myEmbeddings,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "\n",
    "print(faissdb.index.ntotal)\n",
    "print(faissdb.index.d)\n",
    "print(faissdb.index.is_trained)\n"
   ],
   "id": "2d2cb19b987e268f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "496\n",
      "3072\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T08:56:47.831744Z",
     "start_time": "2025-11-10T08:56:46.928482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# Step4-1:\n",
    "#\n",
    "question = '북한의 교육과정'\n",
    "resultDocs = fdb.similarity_search(question)\n",
    "print('유사문서수:', len(resultDocs))\n",
    "print( type(resultDocs), type(resultDocs[0]), type(resultDocs[0].page_content) )"
   ],
   "id": "83657a1ddcd536cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사문서수: 4\n",
      "<class 'list'> <class 'langchain_core.documents.base.Document'> <class 'str'>\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T08:56:52.799333Z",
     "start_time": "2025-11-10T08:56:52.793501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for resultDoc in resultDocs:\n",
    "    print('---' * 10)\n",
    "    print(resultDoc.metadata.get(\"page_label\"), resultDoc.page_content[:100] )\n"
   ],
   "id": "98091f048fcc221d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "351 4. 교육권\n",
      "349\n",
      "IV. 경제적·사회적·문화적 권리 I. 발간개요V. 취약계층VI. 특별사안 II. 요약III. 시민적·정치적 권리\n",
      "및 종교 집단 사이에 이해를 증진시킬 수 있도\n",
      "------------------------------\n",
      "352 2023 북한인권보고서\n",
      "350\n",
      "소학교 때는 김일성, 김정일, 김정숙의 어린 시절을 배우고, 초급중\n",
      "학교에서는 혁명활동을 배우는데, 김정숙과 김정은의 내용을 학기\n",
      "마다 번갈아 가며 \n",
      "------------------------------\n",
      "42 2023 북한인권보고서\n",
      "40\n",
      "명목의 교육비용이 전가되고 있는 것으로 나타났다. 교과서는 ‘교과\n",
      "서 요금’이라는 명목으로 일정 금액을 내야하는 경우가 많으며, 교\n",
      "과서가 모든 학생에\n",
      "------------------------------\n",
      "339 4. 교육권\n",
      "337\n",
      "IV. 경제적·사회적·문화적 권리 I. 발간개요V. 취약계층VI. 특별사안 II. 요약III. 시민적·정치적 권리\n",
      "예산으로 보장하며,338 교과서나 교육비품의 \n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T08:59:09.671818Z",
     "start_time": "2025-11-10T08:59:08.814091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# 4-2: other handle of FAISS DB\n",
    "#\n",
    "fdb2 = FAISS.load_local(\n",
    "    folder_path = faiss_index_path,\n",
    "    embeddings = myEmbeddings,\n",
    "    allow_dangerous_deserialization=True,\n",
    ")\n",
    "print('문서의 수:', fdb2.index.ntotal)\n",
    "print( type(fdb), type(fdb2) )\n",
    "\n",
    "question = '북한의 교육과정'\n",
    "resultDocs = fdb2.similarity_search(question)\n",
    "print('유사문서수:', len(resultDocs))\n",
    "print( type(resultDocs), type(resultDocs[0]), type(resultDocs[0].page_content) )\n",
    "\n",
    "for resultDoc in resultDocs:\n",
    "    print('---' * 10)\n",
    "    print(resultDoc.metadata.get(\"page_label\"), resultDoc.page_content[:100] )\n"
   ],
   "id": "2ba8a45845e77b14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문서의 수: 445\n",
      "<class 'langchain_community.vectorstores.faiss.FAISS'> <class 'langchain_community.vectorstores.faiss.FAISS'>\n",
      "유사문서수: 4\n",
      "<class 'list'> <class 'langchain_core.documents.base.Document'> <class 'str'>\n",
      "------------------------------\n",
      "351 4. 교육권\n",
      "349\n",
      "IV. 경제적·사회적·문화적 권리 I. 발간개요V. 취약계층VI. 특별사안 II. 요약III. 시민적·정치적 권리\n",
      "및 종교 집단 사이에 이해를 증진시킬 수 있도\n",
      "------------------------------\n",
      "352 2023 북한인권보고서\n",
      "350\n",
      "소학교 때는 김일성, 김정일, 김정숙의 어린 시절을 배우고, 초급중\n",
      "학교에서는 혁명활동을 배우는데, 김정숙과 김정은의 내용을 학기\n",
      "마다 번갈아 가며 \n",
      "------------------------------\n",
      "42 2023 북한인권보고서\n",
      "40\n",
      "명목의 교육비용이 전가되고 있는 것으로 나타났다. 교과서는 ‘교과\n",
      "서 요금’이라는 명목으로 일정 금액을 내야하는 경우가 많으며, 교\n",
      "과서가 모든 학생에\n",
      "------------------------------\n",
      "339 4. 교육권\n",
      "337\n",
      "IV. 경제적·사회적·문화적 권리 I. 발간개요V. 취약계층VI. 특별사안 II. 요약III. 시민적·정치적 권리\n",
      "예산으로 보장하며,338 교과서나 교육비품의 \n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T09:00:56.499895Z",
     "start_time": "2025-11-10T09:00:55.866514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#\n",
    "# 4-3\n",
    "# find top 3, print similarity score\n",
    "# List[Document] -> docs[0].metadata, docs[0].page_content\n",
    "# List[Tuple[Document, float]] -> docs[0][0].metadata, docs[0][1]\n",
    "question = '북한의 교육과정'\n",
    "resultDocs = fdb2.similarity_search_with_relevance_scores(question, k=3)\n",
    "print('유사문서수:', len(resultDocs))\n",
    "print( type(resultDocs), type(resultDocs[0]) )\n",
    "print( type(resultDocs[0][0]), type(resultDocs[0][0].page_content) )\n",
    "print( type(resultDocs[0][1]), resultDocs[0][1] )\n",
    "\n",
    "for resultDoc in resultDocs:\n",
    "    print('---' * 10)\n",
    "    print(resultDoc[1], resultDoc[0].page_content[:100])\n",
    "\n",
    "for resultDoc, score in resultDocs:\n",
    "    print('---' * 10)\n",
    "    print(score, len(resultDoc.page_content) )"
   ],
   "id": "fb9801241b4ffa98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유사문서수: 3\n",
      "<class 'list'> <class 'tuple'>\n",
      "<class 'langchain_core.documents.base.Document'> <class 'str'>\n",
      "<class 'numpy.float32'> 0.36758494\n",
      "------------------------------\n",
      "0.36758494 4. 교육권\n",
      "349\n",
      "IV. 경제적·사회적·문화적 권리 I. 발간개요V. 취약계층VI. 특별사안 II. 요약III. 시민적·정치적 권리\n",
      "및 종교 집단 사이에 이해를 증진시킬 수 있도\n",
      "------------------------------\n",
      "0.36240548 2023 북한인권보고서\n",
      "350\n",
      "소학교 때는 김일성, 김정일, 김정숙의 어린 시절을 배우고, 초급중\n",
      "학교에서는 혁명활동을 배우는데, 김정숙과 김정은의 내용을 학기\n",
      "마다 번갈아 가며 \n",
      "------------------------------\n",
      "0.36217612 2023 북한인권보고서\n",
      "40\n",
      "명목의 교육비용이 전가되고 있는 것으로 나타났다. 교과서는 ‘교과\n",
      "서 요금’이라는 명목으로 일정 금액을 내야하는 경우가 많으며, 교\n",
      "과서가 모든 학생에\n",
      "------------------------------\n",
      "0.36758494 1157\n",
      "------------------------------\n",
      "0.36240548 867\n",
      "------------------------------\n",
      "0.36217612 866\n"
     ]
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
